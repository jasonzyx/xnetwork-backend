{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def monitor_resource_usage():\n",
    "#     cpu_percent = psutil.cpu_percent()\n",
    "#     virtual_memory = psutil.virtual_memory()\n",
    "    \n",
    "#     print(f\"CPU Usage: {cpu_percent}%\")\n",
    "#     print(f\"Memory Usage: {virtual_memory.used / (1024 ** 3):.2f} GB used out of {virtual_memory.total / (1024 ** 3):.2f} GB ({virtual_memory.percent}%)\")\n",
    "\n",
    "# monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 52.1%\n",
      "Physical Memory Usage: 12.64%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def monitor_resource_usage():\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    virtual_memory = psutil.virtual_memory()\n",
    "\n",
    "    # Check if 'cached' attribute is available\n",
    "    if hasattr(virtual_memory, 'cached'):\n",
    "        physical_memory_percent = (virtual_memory.used - virtual_memory.cached) / virtual_memory.total * 100\n",
    "    else:\n",
    "        # Alternative calculation if 'cached' is not available\n",
    "        physical_memory_percent = virtual_memory.used / virtual_memory.total * 100\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Physical Memory Usage: {physical_memory_percent:.2f}%\")\n",
    "\n",
    "monitor_resource_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/_swigfaiss.so, 0x0002): Library not loaded: @rpath/libmkl_intel_lp64.1.dylib\n  Referenced from: <4A025BD7-6F1F-3CD5-B319-DE475BB38238> /Users/jasonxu/opt/anaconda3/envs/django42/lib/libfaiss.dylib\n  Reason: tried: '/Users/jasonxu/opt/anaconda3/envs/django42/lib/libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/../../../libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/../../../libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/bin/../lib/libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/bin/../lib/libmkl_intel_lp64.1.dylib' (no such file), '/usr/local/lib/libmkl_intel_lp64.1.dylib' (no such file), '/usr/lib/libmkl_intel_lp64.1.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tk/sf_9rdm11rl75p78wb04fgrc0000gn/T/ipykernel_8659/3182806271.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# linear algebra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[0;31m# accessing directory structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This source code is licensed under the MIT license found in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;31m# @nolint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# not linting this file because it imports * from swigfaiss, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# causes a ton of useless warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/loader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Successfully loaded faiss with AVX2 support.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not load library with AVX2 support due to:\\n{e!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# reset so that we load without AVX2 below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mhas_AVX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_AVX2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# we import * so that the symbol X can be accessed as faiss.X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Do not make changes to this file unless you know what you are doing--modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# the SWIG interface file instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_info\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_swig_python_version_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_swig_python_version_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python 2.7 or later required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/_swigfaiss.so, 0x0002): Library not loaded: @rpath/libmkl_intel_lp64.1.dylib\n  Referenced from: <4A025BD7-6F1F-3CD5-B319-DE475BB38238> /Users/jasonxu/opt/anaconda3/envs/django42/lib/libfaiss.dylib\n  Reason: tried: '/Users/jasonxu/opt/anaconda3/envs/django42/lib/libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/../../../libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/lib/python3.8/site-packages/faiss/../../../libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/bin/../lib/libmkl_intel_lp64.1.dylib' (no such file), '/Users/jasonxu/opt/anaconda3/envs/django42/bin/../lib/libmkl_intel_lp64.1.dylib' (no such file), '/usr/local/lib/libmkl_intel_lp64.1.dylib' (no such file), '/usr/lib/libmkl_intel_lp64.1.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is downloaded from: https://grouplens.org/datasets/movielens/https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "nRowsRead = 500 # specify 'None' if want to read whole file\n",
    "# movie_metadata.csv has 5044 rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "movies = pd.read_csv('../data/movie-lens-small/movies.csv', delimiter=',', nrows = nRowsRead)\n",
    "ratings = pd.read_csv('../data/movie-lens-small/ratings.csv', delimiter=',', nrows = nRowsRead)\n",
    "tags = pd.read_csv('../data/movie-lens-small/tags.csv', delimiter=',', nrows = nRowsRead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(movies.head(5))\n",
    "# display(ratings.head(5))\n",
    "# display(tags.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_titles_batch(titles, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(titles), batch_size):\n",
    "        batch = titles[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        outputs = encoder(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "        print(f\"Processed batch {i // batch_size + 1}/{len(titles) // batch_size + 1}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming movies is a DataFrame with 'title' and 'movieId'\n",
    "batch_size = 128  # Adjust batch size as needed\n",
    "\n",
    "# Process titles in batches and create embeddings\n",
    "embeddings = encode_titles_batch(movies['title'].tolist(), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):    \n",
    "    d = embeddings.shape[1]  # dimension of embeddings\n",
    "    n = embeddings.shape[0]\n",
    "    print(\"Initiatize index\")\n",
    "    print(f'number of records to index: {n}')\n",
    "    # index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_INNER_PRODUCT)\n",
    "    \n",
    "    ######## test ############\n",
    "    # Assuming 'd' is the dimension of your data\n",
    "    quantizer = faiss.IndexFlatIP(d)\n",
    "    nlist = 10  # You can adjust this value as needed\n",
    "    \n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    print(\"Training the index...\")\n",
    "    index.train(embeddings)  # Train the index before adding data\n",
    "    print(\"Training completed.\")\n",
    "    \n",
    "    ######## test ############\n",
    "    \n",
    "    print(\"Adding embeddings to the index...\")\n",
    "    index.add(embeddings)\n",
    "    print(\"Embeddings added to the index.\")\n",
    "\n",
    "    return index\n",
    "# Create FAISS index with these embeddings\n",
    "faiss_index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a mapping of index to movie_id\n",
    "index_to_id = {i: (row['movieId'], row['title']) for i, row in movies[['movieId', 'title']].iterrows()}\n",
    "\n",
    "def encode_query(query, tokenizer, encoder):\n",
    "    print(f\"DEBUG: encoding query: {query}...\")\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    outputs = encoder(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    print(f\"DEBUG: done encoding query: {query}\")\n",
    "\n",
    "    return embeddings\n",
    "    \n",
    "def search_index(query, k, faiss_index, index_to_id, tokenizer, encoder):\n",
    "    query_embedding = encode_query(query, tokenizer, encoder)\n",
    "    print(f\"DEBUG: calling faiss_index with {k}...\")\n",
    "    print(f\"DEBUG: query_embedding dimension...\")\n",
    "    print(query_embedding.shape)\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    print(f\"DEBUG: done calling faiss_index with {k}!\")\n",
    "\n",
    "    # Retrieve movie IDs for the indices\n",
    "    return [(index_to_id[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index('toy story', 5, faiss_index, index_to_id, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"../model-artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming index_to_id is your dictionary\n",
    "with open(f\"{path_prefix}/index_to_id.pickle\", 'wb') as handle:\n",
    "    pickle.dump(index_to_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faiss_index, f'{path_prefix}/faiss_index.idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model\n",
    "encoder.save_pretrained(f\"{path_prefix}/bert_model\")\n",
    "tokenizer.save_pretrained(f\"{path_prefix}/bert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # accessing directory structure\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss \n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"../model-artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "faiss_index = faiss.read_index(f\"{path_prefix}/faiss_index.idx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(f\"{path_prefix}/bert_model\")\n",
    "encoder = BertModel.from_pretrained(f\"{path_prefix}/bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path_prefix}/index_to_id.pickle\", 'rb') as handle:\n",
    "    index_to_id = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query, tokenizer, encoder):\n",
    "    print(f\"DEBUG: encoding query: {query}...\")\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    outputs = encoder(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    print(f\"DEBUG: done encoding query: {query}\")\n",
    "\n",
    "    return embeddings\n",
    "    \n",
    "def search_index(query, k, faiss_index, index_to_id, tokenizer, encoder):\n",
    "    query_embedding = encode_query(query, tokenizer, encoder)\n",
    "    print(f\"DEBUG: calling faiss_index with {k}...\")\n",
    "    print(f\"DEBUG: query_embedding dimension...\")\n",
    "    print(query_embedding.shape)\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    print(f\"DEBUG: done calling faiss_index with {k}!\")\n",
    "\n",
    "    # Retrieve movie IDs for the indices\n",
    "    return [(index_to_id[idx], distances[0][i]) for i, idx in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index('toy', 5, faiss_index, index_to_id, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7181,
     "sourceId": 10279,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29271,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "django42",
   "language": "python",
   "name": "django42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
