{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "\n",
    "# def monitor_resource_usage():\n",
    "#     cpu_percent = psutil.cpu_percent()\n",
    "#     virtual_memory = psutil.virtual_memory()\n",
    "    \n",
    "#     print(f\"CPU Usage: {cpu_percent}%\")\n",
    "#     print(f\"Memory Usage: {virtual_memory.used / (1024 ** 3):.2f} GB used out of {virtual_memory.total / (1024 ** 3):.2f} GB ({virtual_memory.percent}%)\")\n",
    "\n",
    "# monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def monitor_resource_usage():\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    virtual_memory = psutil.virtual_memory()\n",
    "\n",
    "    # Check if 'cached' attribute is available\n",
    "    if hasattr(virtual_memory, 'cached'):\n",
    "        physical_memory_percent = (virtual_memory.used - virtual_memory.cached) / virtual_memory.total * 100\n",
    "    else:\n",
    "        # Alternative calculation if 'cached' is not available\n",
    "        physical_memory_percent = virtual_memory.used / virtual_memory.total * 100\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Physical Memory Usage: {physical_memory_percent:.2f}%\")\n",
    "\n",
    "monitor_resource_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is downloaded from: https://grouplens.org/datasets/movielens/https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "nRowsRead = 500 # specify 'None' if want to read whole file\n",
    "# movie_metadata.csv has 5044 rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "movies = pd.read_csv('../data/movie-lens-small/movies.csv', delimiter=',', nrows = nRowsRead)\n",
    "ratings = pd.read_csv('../data/movie-lens-small/ratings.csv', delimiter=',', nrows = nRowsRead)\n",
    "tags = pd.read_csv('../data/movie-lens-small/tags.csv', delimiter=',', nrows = nRowsRead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "display(movies.head(5))\n",
    "# display(ratings.head(5))\n",
    "# display(tags.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_titles_batch(titles, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(titles), batch_size):\n",
    "        batch = titles[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        outputs = encoder(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "        print(f\"Processed batch {i // batch_size + 1}/{len(titles) // batch_size + 1}\")\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming movies is a DataFrame with 'title' and 'movieId'\n",
    "batch_size = 128  # Adjust batch size as needed\n",
    "\n",
    "# Process titles in batches and create embeddings\n",
    "embeddings = encode_titles_batch(movies['title'].tolist(), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):    \n",
    "    d = embeddings.shape[1]  # dimension of embeddings\n",
    "    n = embeddings.shape[0]\n",
    "    print(\"Initiatize index\")\n",
    "    print(f'number of records to index: {n}')\n",
    "    # index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_INNER_PRODUCT)\n",
    "    \n",
    "    ######## test ############\n",
    "    # Assuming 'd' is the dimension of your data\n",
    "    quantizer = faiss.IndexFlatIP(d)\n",
    "    nlist = 10  # You can adjust this value as needed\n",
    "    \n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    print(\"Training the index...\")\n",
    "    index.train(embeddings)  # Train the index before adding data\n",
    "    print(\"Training completed.\")\n",
    "    \n",
    "    ######## test ############\n",
    "    \n",
    "    print(\"Adding embeddings to the index...\")\n",
    "    index.add(embeddings)\n",
    "    print(\"Embeddings added to the index.\")\n",
    "\n",
    "    return index\n",
    "# Create FAISS index with these embeddings\n",
    "faiss_index = create_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a mapping of index to movie_id\n",
    "index_to_id = {i: (row['movieId'], row['title']) for i, row in movies[['movieId', 'title']].iterrows()}\n",
    "\n",
    "def encode_query(query, tokenizer, encoder):\n",
    "    print(f\"DEBUG: encoding query: {query}...\")\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    outputs = encoder(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    print(f\"DEBUG: done encoding query: {query}\")\n",
    "\n",
    "    return embeddings\n",
    "    \n",
    "def search_index(query, k, faiss_index, index_to_id, tokenizer, encoder):\n",
    "    query_embedding = encode_query(query, tokenizer, encoder)\n",
    "    print(f\"DEBUG: calling faiss_index with {k}...\")\n",
    "    print(f\"DEBUG: query_embedding dimension...\")\n",
    "    print(query_embedding.shape)\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    print(f\"DEBUG: done calling faiss_index with {k}!\")\n",
    "\n",
    "    # Retrieve movie IDs for the indices\n",
    "    return [(index_to_id[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index('toy story', 5, faiss_index, index_to_id, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"../model-artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming index_to_id is your dictionary\n",
    "with open(f\"{path_prefix}/index_to_id.pickle\", 'wb') as handle:\n",
    "    pickle.dump(index_to_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faiss_index, f'{path_prefix}/faiss_index.idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model\n",
    "encoder.save_pretrained(f\"{path_prefix}/bert_model\")\n",
    "tokenizer.save_pretrained(f\"{path_prefix}/bert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # accessing directory structure\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss \n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"../model-artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "faiss_index = faiss.read_index(f\"{path_prefix}/faiss_index.idx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(f\"{path_prefix}/bert_model\")\n",
    "encoder = BertModel.from_pretrained(f\"{path_prefix}/bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path_prefix}/index_to_id.pickle\", 'rb') as handle:\n",
    "    index_to_id = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query, tokenizer, encoder):\n",
    "    print(f\"DEBUG: encoding query: {query}...\")\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    outputs = encoder(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    print(f\"DEBUG: done encoding query: {query}\")\n",
    "\n",
    "    return embeddings\n",
    "    \n",
    "def search_index(query, k, faiss_index, index_to_id, tokenizer, encoder):\n",
    "    query_embedding = encode_query(query, tokenizer, encoder)\n",
    "    print(f\"DEBUG: calling faiss_index with {k}...\")\n",
    "    print(f\"DEBUG: query_embedding dimension...\")\n",
    "    print(query_embedding.shape)\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    print(f\"DEBUG: done calling faiss_index with {k}!\")\n",
    "\n",
    "    # Retrieve movie IDs for the indices\n",
    "    return [(index_to_id[idx], distances[0][i]) for i, idx in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index('toy', 5, faiss_index, index_to_id, tokenizer, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_resource_usage()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7181,
     "sourceId": 10279,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29271,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
